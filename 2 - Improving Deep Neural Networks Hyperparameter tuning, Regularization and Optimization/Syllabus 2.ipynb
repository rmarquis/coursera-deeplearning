{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 2: [Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization](https://www.youtube.com/watch?v=1waHlpKiNyY&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc)\n",
    "\n",
    "Welcome to Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1: Practical aspects of Deep Learning\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "* Recall that different types of initializations lead to different results\n",
    "* Recognize the importance of initialization in complex neural networks.\n",
    "* Recognize the difference between train/dev/test sets\n",
    "* Diagnose the bias and variance issues in your model\n",
    "* Learn when and how to use regularization methods such as dropout or L2 regularization.\n",
    "* Understand experimental issues in deep learning such as Vanishing or Exploding gradients and learn how to deal with them\n",
    "* Use gradient checking to verify the correctness of your backpropagation implementation\n",
    "\n",
    "### Setting up your Machine Learning Application\n",
    "\n",
    "* Train / Dev / Test sets (12 min)\n",
    "* Bias / Variance (8 min)\n",
    "* Basic Recipe for Machine Learning (6 min)\n",
    "\n",
    "### Regularizing your neural network\n",
    "\n",
    "* Regularization (9 min)\n",
    "* Why regularization reduces overfitting? (7 min)\n",
    "* Dropout Regularization (9 min)\n",
    "* Understanding Dropout (7 min)\n",
    "* Other regularization methods (8 min)\n",
    "\n",
    "### Setting up your optimization problem\n",
    "\n",
    "* Normalizing inputs (5 min)\n",
    "* Vanishing / Exploding gradients (6 min)\n",
    "* Weight Initialization for Deep Networks (6 min)\n",
    "* Numerical approximation of gradients (6 min)\n",
    "* Gradient checking (6 min)\n",
    "* Gradient Checking Implementation Notes (5 min)\n",
    "\n",
    "### Practice Questions\n",
    "* Quiz: Practical aspects of deep learning (10 questions)\n",
    "\n",
    "### Programming assignments\n",
    "\n",
    "* Initialization (1h)\n",
    "* Regularization (1h30)\n",
    "* Gradient Checking (1h)\n",
    "\n",
    "#### Programming Assignment: Initialization\n",
    "\n",
    "Welcome to the first assignment of the hyper parameters tuning specialization. It is very important that you regularize your model properly because it could dramatically improve your results.\n",
    "\n",
    "By completing this assignment you will:\n",
    "- Understand that different regularization methods that could help your model.\n",
    "- Implement dropout and see it work on data.\n",
    "- Recognize that a model without regularization gives you a better accuracy on the training set but nor necessarily on the test set.\n",
    "- Understand that you could use both dropout and regularization on your model.\n",
    "\n",
    "This assignment prepares you well for the upcoming assignment. Take your time to complete it and make sure you get the expected outputs when working through the different exercises.\n",
    "\n",
    "#### Programming Assignment: Regularization\n",
    "\n",
    "Welcome to the first assignment of the hyper parameters tuning specialization. It is very important that you regularize your model properly because it could dramatically improve your results.\n",
    "\n",
    "By completing this assignment you will:\n",
    "\n",
    "- Understand that different regularization methods that could help your model.\n",
    "- Implement dropout and see it work on data.\n",
    "- Recognize that a model without regularization gives you a better accuracy on the training set but nor necessarily on the test set.\n",
    "- Understand that you could use both dropout and regularization on your model.\n",
    "\n",
    "This assignment prepares you well for the upcoming assignment. Take your time to complete it and make sure you get the expected outputs when working through the different exercises. \n",
    "\n",
    "#### Programming Assignment: Gradient Checking\n",
    "\n",
    "Welcome to this week's third programming assignment! You will be implementing gradient checking to make sure that your backpropagation implementation is correct. By completing this assignment you will:\n",
    "\n",
    "- Implement gradient checking from scratch.\n",
    "- Understand how to use the difference formula to check your backpropagation implementation.\n",
    "- Recognize that your backpropagation algorithm should give you similar results as the ones you got by computing the difference formula.\n",
    "- Learn how to identify which parameter's gradient was computed incorrectly.\n",
    "\n",
    "Take your time to complete this assignment, and make sure you get the expected outputs when working through the different exercises.\n",
    "\n",
    "### Heroes of Deep Learning (Optional)\n",
    "* Yoshua Bengio interview (25 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2: Optimization algorithms\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "* Remember different optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam\n",
    "* Use random minibatches to accelerate the convergence and improve the optimization\n",
    "* Know the benefits of learning rate decay and apply it to your optimization\n",
    "\n",
    "### Optimization algorithms\n",
    "\n",
    "* Mini-batch gradient descent (11 min)\n",
    "* Understanding mini-batch gradient descent (11 min)\n",
    "* Exponentially weighted averages (5 min)\n",
    "* Understanding exponentially weighted averages (9 min)\n",
    "* Bias correction in exponentially weighted averages (4 min)\n",
    "* Gradient descent with momentum (9 min)\n",
    "* RMSprop (7 min)\n",
    "* Adam optimization algorithm (7 min)\n",
    "* Learning rate decay (6 min)\n",
    "* The problem of local optima (5 min)\n",
    "\n",
    "### Practice Questions\n",
    "\n",
    "* Quiz: Optimization algorithms (10 questions)\n",
    "\n",
    "### Programming assignment\n",
    "\n",
    "#### Optimization\n",
    "Welcome to the optimization's programming assignment of the hyper-parameters tuning specialization. There are many different optimization algorithms you could be using to get you to the minimal cost. Similarly, there are many different paths down this hill to the lowest point.\n",
    "\n",
    "By completing this assignment you will:\n",
    "- Understand the intuition between Adam and RMS prop\n",
    "- Recognize the importance of mini-batch gradient descent\n",
    "- Learn the effects of momentum on the overall performance of your model\n",
    "\n",
    "This assignment prepares you well for the upcoming assignment. Take your time to complete it and make sure you get the expected outputs when working through the different exercises.\n",
    "\n",
    "### Heroes of Deep Learning (Optional)\n",
    "\n",
    "* Yuanqing Lin interview (13 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3: Hyperparameter tuning, Batch Normalization and Programming Frameworks\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "* Master the process of hyperparameter tuning\n",
    "\n",
    "### Hyperparameter tuning\n",
    "\n",
    "* Tuning process (7 min)\n",
    "* Using an appropriate scale to pick hyperparameters (8 min)\n",
    "* Hyperparameters tuning in practice: Pandas vs. Caviar (6 min)\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "* Normalizing activations in a network (8 min)\n",
    "* Fitting Batch Norm into a neural network (12 min)\n",
    "* Why does Batch Norm work? (11 min)\n",
    "* Batch Norm at test time (5 min)\n",
    "\n",
    "### Multi-class classification\n",
    "\n",
    "* Softmax Regression (11 min)\n",
    "* Training a softmax classifier (10 min)\n",
    "\n",
    "### Introduction to programming frameworks\n",
    "\n",
    "* Deep learning frameworks (4 min)\n",
    "* TensorFlow (16 min)\n",
    "\n",
    "### Practice Questions\n",
    "\n",
    "* Quiz: Hyperparameter tuning, Batch Normalization, Programming Frameworks (10 questions)\n",
    "\n",
    "### Programming assignment\n",
    "\n",
    "#### Tensorflow\n",
    "\n",
    "Welcome to the Tensorflow Tutorial! In this notebook you will learn all the basics of Tensorflow. You will implement useful functions and draw the parallel with what you did using Numpy. You will understand what Tensors and operations are, as well as how to execute them in a computation graph.\n",
    "\n",
    "After completing this assignment you will also be able to implement your own deep learning models using Tensorflow. In fact, using our brand new SIGNS dataset, you will build a deep neural network model to recognize numbers from 0 to 5 in sign language with a pretty impressive accuracy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
